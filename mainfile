import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)
warnings.simplefilter(action='ignore', category=UserWarning)


path = '/content/coronary_prediction.csv'
df = pd.read_csv(path)
print (df)

X = df.drop("TenYearCHD", axis=1)
X.head()

y = df["TenYearCHD"]
y.head()


from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)
X_train.shape, X_test.shape, y_train.shape, y_test.shape

#data preprocess
df.info()

# checking on the relevance of imputation. If correlation is high with other IVs, may be used for imputation
def color(val):
    if val > 0.7 or val < -0.7:
        color = 'red'
    else:
        color = 'black'
    return 'color : %s' % color

df.corr().style.applymap(color)
df.isnull().sum()

df_BPMed = df.groupby(['prevalentStroke','prevalentHyp'])
df_BPMed['BPMeds'].sum()

chol_col = ['totChol','BMI']
df_chol = df[chol_col]
df_chol[df_chol['BMI'] == 26]

df[df['BMI'].isna()]

df_diabetes = df.groupby(['diabetes'])
df_diabetes['glucose'].describe()

#missing value
df['education'].mode()
df['education'].fillna(1,inplace=True)

df_smoker = df.groupby(['currentSmoker'])
df_smoker['cigsPerDay'].describe()

# imputation of cigs per day
df['cigsPerDay'].fillna(9999,inplace=True)

index = []
for i,v in enumerate(df['cigsPerDay']):
    if df['cigsPerDay'].loc[i] == 9999:
        if df['currentSmoker'].loc[i] == 1:
            df['cigsPerDay'].loc[i] = 18
            index.append(i)
        else:
            df['cigsPerDay'].loc[i] = 0
            index.append(i)
df.loc[index]

# imputation of BPMeds
df['BPMeds'].fillna(9999,inplace=True)
index = []
for i,v in enumerate(df['BPMeds']):
    if v == 9999:
        if df['prevalentHyp'].loc[i] == 1:
            df['BPMeds'].loc[i] = 1
            index.append(i)
        else:
            df['BPMeds'].loc[i] = 0
            index.append(i)
df.loc[index]

df.totChol.describe()
# impute total cholestrol with mean
df.totChol.fillna(236,inplace=True)
df.BMI.describe()

# impute BMI with mean
df.BMI.fillna(25.8,inplace=True)

df.heartRate.describe()
#impute heartrate with mean
df.heartRate.fillna(75,inplace=True)
df.info()

df['glucose'].fillna(9999,inplace=True)
index = []
for i,v in enumerate(df['glucose']):
    if v == 9999:
        if df['diabetes'].loc[i] == 1:
            df['glucose'].loc[i] = 170
            index.append(i)
        else:
            df['glucose'].loc[i] = 79
            index.append(i)

df_dia = df.loc[index]
df_dia[df_dia['diabetes'] == 1]

from sklearn.svm import SVC
from sklearn.impute import SimpleImputer

# Handle missing values with SimpleImputer
imputer = SimpleImputer(strategy='mean')

X_train_imputed = pd.DataFrame(imputer.fit_transform(X_train), columns=X_train.columns)
X_test_imputed = pd.DataFrame(imputer.transform(X_test), columns=X_test.columns)

#LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.impute import SimpleImputer
import pandas as pd

# Handle missing values with SimpleImputer
imputer = SimpleImputer(strategy='mean')

X_train_imputed = pd.DataFrame(imputer.fit_transform(X_train), columns=X_train.columns)
X_test_imputed = pd.DataFrame(imputer.transform(X_test), columns=X_test.columns)

# Standardize the training data while keeping the column names
ss_train = StandardScaler()
X_train_standardized = pd.DataFrame(ss_train.fit_transform(X_train_imputed), columns=X_train.columns)

# Standardize the test data using the same scaler and column names
X_test_standardized = pd.DataFrame(ss_train.transform(X_test_imputed), columns=X_test.columns)

# Fit a logistic regression model to the standardized training data
model = LogisticRegression()
model.fit(X_train_standardized, y_train)

# Predict on the standardized test data
y_pred = model.predict(X_test_standardized)

# Evaluate the model's performance
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

# Print classification report
print(classification_report(y_test, y_pred))

# Print confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred)
confusion_df = pd.DataFrame(conf_matrix, columns=['Predicted Negative', 'Predicted Positive'], index=['Actual Negative', 'Actual Positive'])
print(confusion_df)

from sklearn.metrics import confusion_matrix

cm = confusion_matrix(y_test, y_pred)
TN, FP, FN, TP = confusion_matrix(y_test, y_pred).ravel()

print('True Positive(TP)  = ', TP)
print('False Positive(FP) = ', FP)
print('True Negative(TN)  = ', TN)
print('False Negative(FN) = ', FN)

accuracy =  (TP + TN) / (TP + FP + TN + FN)
print('Accuracy of the binary classifier = {:0.3f}'.format(accuracy))

from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score
from sklearn.impute import SimpleImputer
import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.svm import LinearSVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier

# Handle missing values with SimpleImputer
imputer = SimpleImputer(strategy='mean')

X_train_imputed = pd.DataFrame(imputer.fit_transform(X_train), columns=X_train.columns)
X_test_imputed = pd.DataFrame(imputer.transform(X_test), columns=X_test.columns)

# Standardize the training data while keeping the column names
ss_train = StandardScaler()
X_train_standardized = pd.DataFrame(ss_train.fit_transform(X_train_imputed), columns=X_train.columns)

# Standardize the test data using the same scaler and column names
X_test_standardized = pd.DataFrame(ss_train.transform(X_test_imputed), columns=X_test.columns)

# Initialize a dictionary to store models
models = {}

# Logistic Regression
logistic_regression_model = LogisticRegression()
logistic_regression_model.fit(X_train_standardized, y_train)
y_pred_lr = logistic_regression_model.predict(X_test_standardized)
accuracy_lr = accuracy_score(y_test, y_pred_lr)
models['Logistic Regression'] = (logistic_regression_model, accuracy_lr)

# Support Vector Machines
svm_model = LinearSVC()
svm_model.fit(X_train_standardized, y_train)
y_pred_svm = svm_model.predict(X_test_standardized)
accuracy_svm = accuracy_score(y_test, y_pred_svm)
models['Support Vector Machines'] = (svm_model, accuracy_svm)

# Decision Trees
decision_tree_model = DecisionTreeClassifier()
decision_tree_model.fit(X_train_standardized, y_train)
y_pred_dt = decision_tree_model.predict(X_test_standardized)
accuracy_dt = accuracy_score(y_test, y_pred_dt)
models['Decision Trees'] = (decision_tree_model, accuracy_dt)

# Random Forest
random_forest_model = RandomForestClassifier()
random_forest_model.fit(X_train_standardized, y_train)
y_pred_rf = random_forest_model.predict(X_test_standardized)
accuracy_rf = accuracy_score(y_test, y_pred_rf)
models['Random Forest'] = (random_forest_model, accuracy_rf)

# Naive Bayes
naive_bayes_model = GaussianNB()
naive_bayes_model.fit(X_train_standardized, y_train)
y_pred_nb = naive_bayes_model.predict(X_test_standardized)
accuracy_nb = accuracy_score(y_test, y_pred_nb)
models['Naive Bayes'] = (naive_bayes_model, accuracy_nb)

# K-Nearest Neighbors
knn_model = KNeighborsClassifier()
knn_model.fit(X_train_standardized, y_train)
y_pred_knn = knn_model.predict(X_test_standardized)
accuracy_knn = accuracy_score(y_test, y_pred_knn)
models['K-Nearest Neighbor'] = (knn_model, accuracy_knn)

# Print the accuracy of each model
for model_name, (model, accuracy) in models.items():
    print(f"{model_name} Accuracy: {accuracy:.2f}")
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import classification_report, confusion_matrix

# Initialize a dictionary to store models
models = {
    'Logistic Regression': logistic_regression_model,
    'Support Vector Machines': svm_model,
    'Decision Trees': decision_tree_model,
    'Random Forest': random_forest_model,
    'Naive Bayes': naive_bayes_model,
    'K-Nearest Neighbor': knn_model
}

# Loop through each model
for model_name, model in models.items():
    # Make predictions
    y_pred = model.predict(X_test_standardized)

    # Classification Report
    report = classification_report(y_test, y_pred)
    print(f"{model_name} Classification Report:\n{report}")

    # Calculate sensitivity (True Positive Rate)
    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()
    sensitivity = tp / (tp + fn)
    print(f"{model_name} Sensitivity (True Positive Rate): {sensitivity:.2f}")

    # Heatmap of Confusion Matrix
    confusion = confusion_matrix(y_test, y_pred)
    plt.figure(figsize=(8, 6))
    sns.heatmap(confusion, annot=True, fmt="d", cmap="Blues",
                xticklabels=["Negative", "Positive"],
                yticklabels=["Negative", "Positive"])
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.title(f'{model_name} Confusion Matrix')
    plt.show()

import matplotlib.pyplot as plt 
# Define the classifier names and their accuracies 
model_names = ["Logistic Regression", "SVM", "Decision Trees", "Random Forest", "Naive Bayes", "K-Nearest Neighbor"] 
accuracies = [0.85, 0.85, 0.77, 0.85, 0.82, 0.84] 

# Create a bar chart to compare classifier performance 
plt.figure(figsize=(10, 6)) 
plt.barh(model_names, accuracies, color='skyblue') 
plt.xlabel('Accuracy') 
plt.title('Classifier Performance Comparison') 
plt.xlim(0, 1) 
plt.gca().invert_yaxis() # Reverse the order for better visualization 
plt.show()

import matplotlib.pyplot as plt
import numpy as np

# Define the classifier names and their accuracy, precision, and recall values
model_names = ["Logistic Regression", "SVM", "Decision Trees", "Random Forest", "Naive Bayes", "K-Nearest Neighbor"]
accuracies = [0.85, 0.85, 0.77, 0.85, 0.82, 0.84]  
precisions = [0.85, 0.85, 0.86, 0.86, 0.86, 0.86]  
recalls =    [1.00, 1.00, 0.86, 0.99, 0.93, 0.97]   

# Set the width of the bars
bar_width = 0.2

# Create an array for the x-axis positions of bars
index = np.arange(len(model_names))

# Create a bar chart to compare classifier performance (accuracy, precision, recall)
plt.figure(figsize=(12, 6))

# Plot accuracy
plt.barh(index, accuracies, bar_width, label='Accuracy', color='#176aa0', edgecolor='black')

# Plot precision
plt.barh(index + bar_width, precisions, bar_width, label='Precision', color='#1ce3bd', edgecolor='black')

# Plot recall
plt.barh(index + 2 * bar_width, recalls, bar_width, label='Recall', color='#de552c', edgecolor='black')

plt.xlabel('Score')
plt.title('Classifier Performance Comparison')
plt.xlim(0, 1)
plt.xticks(np.arange(0, 1.1, 0.1))
plt.yticks(index + bar_width, model_names)
plt.gca().invert_yaxis()

# Add a legend
plt.legend(loc='lower right')

# Show the plot
plt.tight_layout()
plt.show()
